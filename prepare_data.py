import polars as pl
import os
from datetime import datetime

# --- Configuration ---
class Config:
    RAW_DATA_PATH = "data.txt"  # Le fichier source (ignor√© par git)
    PROCESSED_DIR = "data/processed"
    
    # Colonnes num√©riques √† nettoyer (Format FR ',' -> '.')
    FLOAT_COLS = [
        "Montant", "TauxImpNb_RB", "TauxImpNB_CPM", 
        "ScoringFP1", "ScoringFP2", "ScoringFP3",
        "DiffDateTr1", "DiffDateTr2", "DiffDateTr3",
        "CA3TRetMtt", "CA3TR", "EcartNumCheq", 
        "NbrMagasin3J", "D2CB"
    ]
    
    # Colonnes enti√®res
    INT_COLS = ["FlagImpaye", "CodeDecision", "VerifianceCPT1", "VerifianceCPT2", "VerifianceCPT3"]
    
    # Split Temporel (Selon le sujet)
    SPLIT_DATE = datetime(2017, 9, 1)

def main():
    print(f"üöÄ D√©marrage du pipeline de pr√©paration des donn√©es...")
    
    # 1. V√©rification de l'existence du fichier
    if not os.path.exists(Config.RAW_DATA_PATH):
        raise FileNotFoundError(f"‚ùå Fichier introuvable : {Config.RAW_DATA_PATH}. "
                                f"Assurez-vous de l'avoir t√©l√©charg√© √† la racine.")

    # 2. Chargement Robuste ("Dirty CSV Strategy")
    print("--- Chargement et Nettoyage (Polars) ---")
    q = (
        pl.scan_csv(
            Config.RAW_DATA_PATH, 
            separator=";", 
            infer_schema_length=0 # Tout lire en String pour √©viter les crashs de parsing
        )
        # Filtrer la ligne d'en-t√™te parasite (header au milieu du fichier)
        .filter(pl.col("ZIBZIN") != "ZIBZIN")
        
        # Nettoyage et Casting des Floats
        .with_columns([
            pl.col(col).str.replace(",", ".").cast(pl.Float64) 
            for col in Config.FLOAT_COLS
        ])
        # Casting des Ints
        .with_columns([
            pl.col(col).cast(pl.Int64) 
            for col in Config.INT_COLS
        ])
        # Parsing de la Date
        .with_columns(
            pl.col("DateTransaction").str.to_datetime("%Y-%m-%d %H:%M:%S")
        )
    )
    
    # Ex√©cution du plan
    df = q.collect()
    print(f"‚úÖ Donn√©es brutes charg√©es. Dimensions : {df.shape}")

    # 3. Feature Engineering & Nettoyage Colonnes
    print("--- Feature Engineering ---")
    df_clean = df.with_columns(
        pl.col("DateTransaction").dt.hour().alias("HourOfDay")
    )
    
    # Suppression des colonnes inutiles ou interdites (CodeDecision = Leakage)
    cols_to_drop = ["ZIBZIN", "IDAvisAutorisationCheque", "Heure", "CodeDecision"]
    existing_cols_to_drop = [c for c in cols_to_drop if c in df_clean.columns]
    df_clean = df_clean.drop(existing_cols_to_drop)
    
    print(f"Colonnes supprim√©es : {existing_cols_to_drop}")

    # 4. Split Train / Test
    print("--- Split Temporel (Train < Sept 2017 <= Test) ---")
    train_df = df_clean.filter(pl.col("DateTransaction") < Config.SPLIT_DATE)
    test_df = df_clean.filter(pl.col("DateTransaction") >= Config.SPLIT_DATE)
    
    print(f"Train set : {train_df.shape[0]} lignes")
    print(f"Test set  : {test_df.shape[0]} lignes")

    # 5. Sauvegarde en Parquet
    os.makedirs(Config.PROCESSED_DIR, exist_ok=True)
    
    train_path = f"{Config.PROCESSED_DIR}/train.parquet"
    test_path = f"{Config.PROCESSED_DIR}/test.parquet"
    
    train_df.write_parquet(train_path)
    test_df.write_parquet(test_path)
    
    print(f"‚úÖ Sauvegarde termin√©e :")
    print(f"   -> {train_path}")
    print(f"   -> {test_path}")
    print("üèÅ Pipeline termin√© avec succ√®s.")

if __name__ == "__main__":
    main()